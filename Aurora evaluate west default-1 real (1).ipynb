{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from aurora import Batch, Metadata, AuroraSmallPretrained\n",
    "from datetime import datetime, timedelta\n",
    "from torch.nn import L1Loss\n",
    "import torch.optim as optim\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aurora import Aurora, Batch, Metadata\n",
    "from aurora import Batch as BaseBatch\n",
    "from aurora import Metadata as BaseMetadata\n",
    "from aurora.batch import interpolate\n",
    "\n",
    "# Import dependencies\n",
    "import contextlib\n",
    "from functools import partial\n",
    "import dataclasses\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "from torch.nn import L1Loss  # = MAE: Mean Absolute Error = '.abs().mean()'\n",
    "from torch.nn import Module  # for custom WeightedMAELoss (Aurora loss)\n",
    "import torch.nn.functional as F  # for downsample batch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from peft import get_peft_model\n",
    "import tempfile\n",
    "import requests\n",
    "import pickle\n",
    "import dask\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import zarr\n",
    "import gcsfs\n",
    "from typing import Union, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aurora core\n",
    "from aurora import Aurora, Batch, Metadata\n",
    "from aurora.batch import interpolate\n",
    "\n",
    "# Dataset & dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Model training utils\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import L1Loss, Module\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast\n",
    "\n",
    "\n",
    "# LoRA PEFT\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# Data processing\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Filesystem + storage\n",
    "import gcsfs\n",
    "import zarr\n",
    "\n",
    "# General utilities\n",
    "from typing import Union, Callable\n",
    "from datetime import datetime, timedelta\n",
    "import dataclasses\n",
    "import tempfile\n",
    "import contextlib\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# Avoid CUDA error: invalid configuration argument on F.scaled_dot_product_attention\n",
    "# https://stackoverflow.com/questions/77343471/pytorch-cuda-error-invalid-configuration-argument\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "\n",
    "\n",
    "# Constants\n",
    "STATIC_VARS_HF_URL = \"https://huggingface.co/microsoft/aurora/resolve/main/aurora-0.25-static.pickle\"\n",
    "GCS_URL = \"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721.zarr/\"\n",
    "LEAD_TIME = pd.Timedelta(\"6h\")\n",
    "AURORA_PRESSURE_LEVELS = [50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000]\n",
    "AURORA_VARIABLE_NAMES = {\n",
    "    \"surface\": [\n",
    "        '10u',\n",
    "        '10v',\n",
    "        '2t',\n",
    "        'msl',\n",
    "    ],\n",
    "    \"atmospheric\": [\n",
    "        't',\n",
    "        'u',\n",
    "        'v',\n",
    "        'q',\n",
    "        'z',\n",
    "    ],\n",
    "    \"static\": [\n",
    "        'z',  # geopotential_at_sea_level\n",
    "        'lsm',  # land_sea_mask\n",
    "        'slt',  # soil_type\n",
    "    ]\n",
    "}\n",
    "VARIABLES_STATISTICS: dict[str, tuple[float, float]] = { # mean, std (location, scale)\n",
    "    'z': (-1386.496, 58844.67),\n",
    "    'lsm': (0.0, 1.0),\n",
    "    'slt': (0.0, 7.0),\n",
    "    '2t': (278.514, 21.22036),\n",
    "    '10u': (-0.05135059, 5.547512),\n",
    "    '10v': (0.189158, 4.765339),\n",
    "    'msl': (100957.8, 1332.246),\n",
    "    'z_50': (199373.0, 5875.553),\n",
    "    'z_100': (157642.1, 5510.64),\n",
    "    'z_150': (133141.4, 5823.912),\n",
    "    'z_200': (115330.0, 5820.169),\n",
    "    'z_250': (101223.1, 5536.585),\n",
    "    'z_300': (89414.15, 5091.916),\n",
    "    'z_400': (69980.38, 4150.851),\n",
    "    'z_500': (54115.37, 3353.187),\n",
    "    'z_600': (40648.33, 2695.808),\n",
    "    'z_700': (28928.82, 2136.436),\n",
    "    'z_850': (13749.78, 1470.321),\n",
    "    'z_925': (7015.005, 1228.997),\n",
    "    'z_1000': (738.1545, 1072.307),\n",
    "    'u_50': (5.653076, 15.29281),\n",
    "    'u_100': (10.27951, 13.52611),\n",
    "    'u_150': (13.54061, 16.04335),\n",
    "    'u_200': (14.20915, 17.6763),\n",
    "    'u_250': (13.34584, 17.9671),\n",
    "    'u_300': (11.80173, 17.11917),\n",
    "    'u_400': (8.817291, 14.34276),\n",
    "    'u_500': (6.563273, 11.98419),\n",
    "    'u_600': (4.814521, 10.33421),\n",
    "    'u_700': (3.345237, 9.168821),\n",
    "    'u_850': (1.418379, 8.188043),\n",
    "    'u_925': (0.6172657, 7.940808),\n",
    "    'u_1000': (-0.03328723, 6.141778),\n",
    "    'v_50': (0.004226111, 7.058931),\n",
    "    'v_100': (0.01411897, 7.47931),\n",
    "    'v_150': (-0.03697671, 9.57199),\n",
    "    'v_200': (-0.04507801, 11.88069),\n",
    "    'v_250': (-0.02980338, 13.38039),\n",
    "    'v_300': (-0.0229477, 13.34044),\n",
    "    'v_400': (-0.01771003, 11.22955),\n",
    "    'v_500': (-0.02387986, 9.181708),\n",
    "    'v_600': (-0.02716674, 7.803569),\n",
    "    'v_700': (0.02153583, 6.87104),\n",
    "    'v_850': (0.142815, 6.264443),\n",
    "    'v_925': (0.205348, 6.470644),\n",
    "    'v_1000': (0.1867637, 5.308203),\n",
    "    't_50': (212.4864, 10.26284),\n",
    "    't_100': (208.4042, 12.52901),\n",
    "    't_150': (213.3201, 8.928709),\n",
    "    't_200': (218.0615, 7.189547),\n",
    "    't_250': (222.771, 8.529282),\n",
    "    't_300': (228.8696, 10.71679),\n",
    "    't_400': (242.1368, 12.69102),\n",
    "    't_500': (252.9492, 13.06447),\n",
    "    't_600': (261.1347, 13.42046),\n",
    "    't_700': (267.401, 14.76523),\n",
    "    't_850': (274.56, 15.5888),\n",
    "    't_925': (277.3572, 16.08798),\n",
    "    't_1000': (281.013, 17.13983),\n",
    "    'q_50': (2.67818e-06, 3.571687e-07),\n",
    "    'q_100': (2.633677e-06, 5.703754e-07),\n",
    "    'q_150': (5.254625e-06, 3.794077e-06),\n",
    "    'q_200': (1.940632e-05, 2.267534e-05),\n",
    "    'q_250': (5.773618e-05, 7.446644e-05),\n",
    "    'q_300': (0.0001273861, 0.0001684361),\n",
    "    'q_400': (0.0003855659, 0.0005078644),\n",
    "    'q_500': (0.0008529599, 0.001079294),\n",
    "    'q_600': (0.001541429, 0.001769722),\n",
    "    'q_700': (0.002431637, 0.002549169),\n",
    "    'q_850': (0.004575618, 0.004112368),\n",
    "    'q_925': (0.006033134, 0.005071058),\n",
    "    'q_1000': (0.007030342, 0.005913548)\n",
    "}\n",
    "\n",
    "class Xaurora(Aurora):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        self.autocast = kwargs.pop(\"autocast\", True)  # Remove 'autocast' from kwargs\n",
    "        self.autocast_dtype = torch.bfloat16\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def forward(self, batch: Batch, lead_time: timedelta) -> Batch:\n",
    "        member_id = batch.metadata.member_id\n",
    "\n",
    "        # Move batch to correct dtype and device\n",
    "        p = next(self.parameters())\n",
    "        batch = batch.type(p.dtype)\n",
    "        batch = batch.normalise()\n",
    "        batch = batch.crop(patch_size=self.encoder.patch_size)\n",
    "        batch = batch.to(p.device)\n",
    "\n",
    "        # Determine resolution of latent patch representation\n",
    "        H, W = batch.spatial_shape\n",
    "        patch_res = (\n",
    "            self.encoder.latent_levels,\n",
    "            H // self.encoder.patch_size,\n",
    "            W // self.encoder.patch_size,\n",
    "        )\n",
    "\n",
    "        # Repeat static variables to match batch/time dims\n",
    "        B, T = next(iter(batch.surf_vars.values())).shape[:2]\n",
    "        batch = dataclasses.replace(\n",
    "            batch,\n",
    "            static_vars={k: v[None, None].repeat(B, T, 1, 1) for k, v in batch.static_vars.items()},\n",
    "        )\n",
    "\n",
    "        # Encode inputs\n",
    "        x = self.encoder(batch, lead_time=lead_time)\n",
    "\n",
    "        # Apply backbone with autocast if enabled\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=self.autocast_dtype) if self.autocast else contextlib.nullcontext():\n",
    "            x = self.backbone(\n",
    "                x,\n",
    "                lead_time=lead_time,\n",
    "                patch_res=patch_res,\n",
    "                rollout_step=batch.metadata.rollout_step,\n",
    "            )\n",
    "\n",
    "        # Decode forecast\n",
    "        pred = self.decoder(\n",
    "            x,\n",
    "            batch,\n",
    "            lead_time=lead_time,\n",
    "            patch_res=patch_res,\n",
    "        )\n",
    "\n",
    "        # Wrap in Batch object and restore metadata\n",
    "        pred = Batch.from_aurora_batch(pred, member_id=member_id)\n",
    "        assert pred.metadata.member_id == batch.metadata.member_id, \"Member ID mismatch.\"\n",
    "\n",
    "        # Remove temporal dims from static variables\n",
    "        pred = dataclasses.replace(\n",
    "            pred,\n",
    "            static_vars={k: v[0, 0] for k, v in batch.static_vars.items()},\n",
    "        )\n",
    "\n",
    "        # Ensure output shape includes time dim (1 step)\n",
    "        pred = dataclasses.replace(\n",
    "            pred,\n",
    "            surf_vars={k: v[:, None] for k, v in pred.surf_vars.items()},\n",
    "            atmos_vars={k: v[:, None] for k, v in pred.atmos_vars.items()},\n",
    "            metadata=dataclasses.replace(\n",
    "                pred.metadata,\n",
    "                rollout_step=batch.metadata.rollout_step + 1,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Unnormalize the output\n",
    "        pred = pred.unnormalise()\n",
    "        return pred\n",
    "\n",
    "    \n",
    "XauroraSmall = partial(\n",
    "    Xaurora,\n",
    "    encoder_depths=(2, 6, 2),\n",
    "    encoder_num_heads=(4, 8, 16),\n",
    "    decoder_depths=(2, 6, 2),\n",
    "    decoder_num_heads=(16, 8, 4),\n",
    "    embed_dim=256,\n",
    "    num_heads=8,\n",
    "    use_lora=False,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Metadata(BaseMetadata):\n",
    "    \"\"\"Metadata in a batch.\n",
    "\n",
    "    Args:\n",
    "        lat (:class:`torch.Tensor`): Latitudes.\n",
    "        lon (:class:`torch.Tensor`): Longitudes.\n",
    "        time (tuple[datetime, ...]): For every batch element, the time.\n",
    "        atmos_levels (tuple[int | float, ...]): Pressure levels for the atmospheric variables in\n",
    "            hPa.\n",
    "        rollout_step (int, optional): How many roll-out steps were used to produce this prediction.\n",
    "            If equal to `0`, which is the default, then this means that this is not a prediction,\n",
    "            but actual data. This field is automatically populated by the model and used to use a\n",
    "            separate LoRA for every roll-out step. Generally, you are safe to ignore this field.\n",
    "        member_id (int, optional): The member ID of the ensemble member. It defaults to `0`.\n",
    "    \"\"\"\n",
    "\n",
    "    lat: torch.Tensor\n",
    "    lon: torch.Tensor\n",
    "    time: tuple[datetime, ...]\n",
    "    atmos_levels: tuple[int | float, ...]\n",
    "    rollout_step: int = 0\n",
    "    member_id: int|list[int] = None\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Batch:\n",
    "    \"\"\"A batch of data.\n",
    "\n",
    "    Args:\n",
    "        surf_vars (dict[str, :class:`torch.Tensor`]): Surface-level variables with shape\n",
    "            `(b, t, h, w)`.\n",
    "        static_vars (dict[str, :class:`torch.Tensor`]): Static variables with shape `(h, w)`.\n",
    "        atmos_vars (dict[str, :class:`torch.Tensor`]): Atmospheric variables with shape\n",
    "            `(b, t, c, h, w)`.\n",
    "        metadata (:class:`Metadata`): Metadata associated to this batch.\n",
    "    \"\"\"\n",
    "\n",
    "    surf_vars: dict[str, torch.Tensor]\n",
    "    static_vars: dict[str, torch.Tensor]\n",
    "    atmos_vars: dict[str, torch.Tensor]\n",
    "    metadata: Metadata\n",
    "    \n",
    "    @property\n",
    "    def spatial_shape(self) -> tuple[int, int]:\n",
    "        \"\"\"Get the spatial shape from an arbitrary surface-level variable.\"\"\"\n",
    "        return next(iter(self.surf_vars.values())).shape[-2:]\n",
    "    \n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"Save the batch to a file.\"\"\"\n",
    "        torch.save(self, path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(path: str) -> \"Batch\":\n",
    "        \"\"\"Load a batch from a file.\"\"\"\n",
    "        return torch.load(path, weights_only=False)\n",
    "    \n",
    "    def fillna(self, how: str=\"spatial_conv\", **how_kwargs):\n",
    "        if how == \"spatial_conv\":\n",
    "            return Batch(\n",
    "                surf_vars={k: fillna_spatial_mean_conv(v, **how_kwargs) for k, v in self.surf_vars.items()},\n",
    "                static_vars={k: fillna_spatial_mean_conv(v, **how_kwargs) for k, v in self.static_vars.items()},\n",
    "                atmos_vars={k: fillna_spatial_mean_conv(v, **how_kwargs) for k, v in self.atmos_vars.items()},\n",
    "                metadata=self.metadata,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fillna method {how}.\")\n",
    "    \n",
    "    def normalise(\n",
    "        self, \n",
    "        stats: dict[str, tuple[float, float]]=VARIABLES_STATISTICS\n",
    "    ) -> \"Batch\":\n",
    "        assert all(\n",
    "            k in stats.keys() for k in self.surf_vars.keys()\n",
    "        ), \"Not all surface variables have statistics.\"\n",
    "        return Batch(\n",
    "            surf_vars={\n",
    "                k: normalise_surf_var(v, k, stats=stats) for k, v in self.surf_vars.items()\n",
    "            },\n",
    "            static_vars={\n",
    "                k: normalise_surf_var(v, k, stats=stats) for k, v in self.static_vars.items()\n",
    "            },\n",
    "            atmos_vars={\n",
    "                k: normalise_atmos_var(v, k, self.metadata.atmos_levels, stats=stats)\n",
    "                for k, v in self.atmos_vars.items()\n",
    "            },\n",
    "            metadata=self.metadata,\n",
    "        )\n",
    "    \n",
    "    def unnormalise(\n",
    "        self, \n",
    "        stats: dict[str, tuple[float, float]]=VARIABLES_STATISTICS\n",
    "    ) -> \"Batch\":\n",
    "        assert all(\n",
    "            k in stats.keys() for k in self.surf_vars.keys()\n",
    "        ), \"Not all surface variables have statistics.\"\n",
    "        return Batch(\n",
    "            surf_vars={\n",
    "                k: unnormalise_surf_var(v, k, stats=stats) for k, v in self.surf_vars.items()\n",
    "            },\n",
    "            static_vars={\n",
    "                k: unnormalise_surf_var(v, k, stats=stats) for k, v in self.static_vars.items()\n",
    "            },\n",
    "            atmos_vars={\n",
    "                k: unnormalise_atmos_var(v, k, self.metadata.atmos_levels, stats=stats)\n",
    "                for k, v in self.atmos_vars.items()\n",
    "            },\n",
    "            metadata=self.metadata,\n",
    "        )\n",
    "        \n",
    "    def crop(self, patch_size: int) -> \"Batch\":\n",
    "        \"\"\"Crop the variables in the batch to patch size `patch_size`.\"\"\"\n",
    "        h, w = self.spatial_shape\n",
    "\n",
    "        if w % patch_size != 0:\n",
    "            raise ValueError(\"Width of the data must be a multiple of the patch size.\")\n",
    "\n",
    "        if h % patch_size == 0:\n",
    "            return self\n",
    "        \n",
    "        elif h % patch_size == 1:\n",
    "            return Batch(\n",
    "                surf_vars={k: v[..., :-1, :] for k, v in self.surf_vars.items()},\n",
    "                static_vars={k: v[..., :-1, :] for k, v in self.static_vars.items()},\n",
    "                atmos_vars={k: v[..., :-1, :] for k, v in self.atmos_vars.items()},\n",
    "                metadata=Metadata(\n",
    "                    lat=self.metadata.lat[:-1],\n",
    "                    lon=self.metadata.lon,\n",
    "                    atmos_levels=self.metadata.atmos_levels,\n",
    "                    time=self.metadata.time,\n",
    "                    rollout_step=self.metadata.rollout_step,\n",
    "                    member_id=self.metadata.member_id,\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"There can at most be one latitude too many, \"\n",
    "                f\"but there are {h % patch_size} too many.\"\n",
    "            )\n",
    "            \n",
    "    def crop_right(self, n: int) -> \"Batch\":\n",
    "        \"\"\"Crop the rightmost `n` columns of the variables in the batch.\"\"\"\n",
    "        return Batch(\n",
    "            surf_vars={k: v[..., :-n, :] for k, v in self.surf_vars.items()},\n",
    "            static_vars={k: v[..., :-n, :] for k, v in self.static_vars.items()},\n",
    "            atmos_vars={k: v[..., :-n, :] for k, v in self.atmos_vars.items()},\n",
    "            metadata=Metadata(\n",
    "                lat=self.metadata.lat[:-n],\n",
    "                lon=self.metadata.lon,\n",
    "                atmos_levels=self.metadata.atmos_levels,\n",
    "                time=self.metadata.time,\n",
    "                rollout_step=self.metadata.rollout_step,\n",
    "                member_id=self.metadata.member_id,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def _fmap(self, f: Callable[[torch.Tensor], torch.Tensor]) -> \"Batch\":\n",
    "        return Batch(\n",
    "            surf_vars={k: f(v) for k, v in self.surf_vars.items()},\n",
    "            static_vars={k: f(v) for k, v in self.static_vars.items()},\n",
    "            atmos_vars={k: f(v) for k, v in self.atmos_vars.items()},\n",
    "            metadata=Metadata(\n",
    "                lat=f(self.metadata.lat),\n",
    "                lon=f(self.metadata.lon),\n",
    "                atmos_levels=self.metadata.atmos_levels,\n",
    "                time=self.metadata.time,\n",
    "                rollout_step=self.metadata.rollout_step,\n",
    "                member_id=self.metadata.member_id,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def to(self, device: str | torch.device) -> \"Batch\":\n",
    "        \"\"\"Move the batch to another device.\"\"\"\n",
    "        return self._fmap(lambda x: x.to(device))\n",
    "\n",
    "    def type(self, t: type) -> \"Batch\":\n",
    "        \"\"\"Convert everything to type `t`.\"\"\"\n",
    "        return self._fmap(lambda x: x.type(t))\n",
    "\n",
    "    def regrid(self, res: float) -> \"Batch\":\n",
    "        \"\"\"Regrid the batch to a `res` degrees resolution.\n",
    "\n",
    "        This results in `float32` data on the CPU.\n",
    "\n",
    "        This function is not optimised for either speed or accuracy. Use at your own risk.\n",
    "        \"\"\"\n",
    "\n",
    "        shape = (round(180 / res) + 1, round(360 / res))\n",
    "        lat_new = torch.from_numpy(np.linspace(90, -90, shape[0]))\n",
    "        lon_new = torch.from_numpy(np.linspace(0, 360, shape[1], endpoint=False))\n",
    "        interpolate_res = partial(\n",
    "            interpolate,\n",
    "            lat=self.metadata.lat,\n",
    "            lon=self.metadata.lon,\n",
    "            lat_new=lat_new,\n",
    "            lon_new=lon_new,\n",
    "        )\n",
    "\n",
    "        return Batch(\n",
    "            surf_vars={k: interpolate_res(v) for k, v in self.surf_vars.items()},\n",
    "            static_vars={k: interpolate_res(v) for k, v in self.static_vars.items()},\n",
    "            atmos_vars={k: interpolate_res(v) for k, v in self.atmos_vars.items()},\n",
    "            metadata=Metadata(\n",
    "                lat=lat_new,\n",
    "                lon=lon_new,\n",
    "                atmos_levels=self.metadata.atmos_levels,\n",
    "                time=self.metadata.time,\n",
    "                rollout_step=self.metadata.rollout_step,\n",
    "                member_id=self.metadata.member_id,\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "    def downsample(self, factor: int) -> \"Batch\":\n",
    "        \"\"\"Downsample batch by factor using Batch.regrid.\n",
    "        \n",
    "        See notes about using it carefully in .regrid\"\"\"\n",
    "        \n",
    "        # detach and to cpu \n",
    "        if self.metadata.lat.device != torch.device('cpu'):\n",
    "            out = self._fmap(lambda x: x.detach().cpu())\n",
    "        else:\n",
    "            out = self\n",
    "        \n",
    "        _, W = out.spatial_shape\n",
    "        current_resolution = 360 / W\n",
    "        new_resolution = current_resolution * factor\n",
    "        \n",
    "        return out.regrid(new_resolution)\n",
    "    \n",
    "    @classmethod        \n",
    "    def from_aurora_batch(cls, aurora_batch: BaseBatch, **metadata_kwargs) -> \"Batch\":\n",
    "        return cls(\n",
    "            surf_vars=aurora_batch.surf_vars,\n",
    "            static_vars=aurora_batch.static_vars,\n",
    "            atmos_vars=aurora_batch.atmos_vars,\n",
    "            metadata=Metadata(\n",
    "                lat=aurora_batch.metadata.lat,\n",
    "                lon=aurora_batch.metadata.lon,\n",
    "                time=aurora_batch.metadata.time,\n",
    "                atmos_levels=aurora_batch.metadata.atmos_levels,\n",
    "                rollout_step=aurora_batch.metadata.rollout_step,\n",
    "                **metadata_kwargs\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "class WeightedMAELoss(Module):  # custom loss functions in torch should inherit from Module\n",
    "    \"\"\"\n",
    "    The original loss function that the Aurora team used as their training objective for\n",
    "    both pre-training and fine-tuning.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        gamma=2.0,  # dataset weight\n",
    "        alpha=0.25,  # overall surface loss weight\n",
    "        beta=1.0,  # overall atmospheric loss weight\n",
    "        surf_var_weights={},  # to store weights for individual variable weighting\n",
    "        atmos_var_weights={},  # to store weights for individual variable weighting\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.l1loss = L1Loss(reduction='sum')  # sum of AE for individual variables that we'll normalize\n",
    "\n",
    "        # Set default variable weights if not provided\n",
    "        self.surf_var_weights = surf_var_weights if surf_var_weights is not None else {\n",
    "            \"2t\": 1.0, \"10u\": 1.0, \"10v\": 1.0, \"msl\": 1.0\n",
    "        }\n",
    "        self.atmos_var_weights = atmos_var_weights if atmos_var_weights is not None else {\n",
    "            \"z\": 1.0, \"q\": 1.0, \"t\": 1.0, \"u\": 1.0, \"v\": 1.0\n",
    "        }\n",
    "\n",
    "    def forward(self, pred_batch: Batch, target_batch: Batch) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Main method to compute the loss.\n",
    "\n",
    "        Gets spatial dimensions (H, W) from predictions, assuming:\n",
    "        - pred_batch.surf_vars has shape (batch, time, H, W)\n",
    "        - atmos_vars has shape (batch, time, C, H, W)\n",
    "        - time dimension for targets is 1.\n",
    "        - H and W are consistent.\n",
    "\n",
    "        pred_surf_shape example: (B, T, H, W) where T for pred is 1\n",
    "        pred_atmos_shape example: (B, T, C, H, W) where T for pred is 1\n",
    "        \"\"\"\n",
    "        # Get dimensions from the *predicted* batch, as target batch is constructed based on it\n",
    "        # next(iter(...)) is used to get the first variable's tensor\n",
    "        _, _, H, W = next(iter(pred_batch.surf_vars.values())).shape\n",
    "\n",
    "        # Determine number of surface and atmospheric variables\n",
    "        VS = len(pred_batch.surf_vars)\n",
    "        VA = len(pred_batch.atmos_vars)\n",
    "\n",
    "        # Use a tensor from the pred_batch to infer the correct device\n",
    "        device = pred_batch.surf_vars[list(pred_batch.surf_vars.keys())[0]].device\n",
    "\n",
    "        # Initialize total loss components\n",
    "        surface_loss_sum = torch.tensor(0.0, device=device)\n",
    "        atmospheric_loss_sum = torch.tensor(0.0, device=device)\n",
    "\n",
    "        # Calculate Surface Loss Component.\n",
    "        for k in pred_batch.surf_vars:\n",
    "            # Get the variable-specific weight w^S_k\n",
    "            w_S_k = self.surf_var_weights.get(k, 1.0)  # default to 1.0 if not found\n",
    "\n",
    "            # L1Loss with reduction='sum' sums up all absolute differences for the variable\n",
    "            # Then we normalize by H*W\n",
    "            var_mae = self.l1loss(pred_batch.surf_vars[k], target_batch.surf_vars[k]) / (H * W)\n",
    "            surface_loss_sum += w_S_k * var_mae\n",
    "\n",
    "        # Calculate Atmospheric Loss Component\n",
    "        for k in pred_batch.atmos_vars:\n",
    "            # Get the variable-specific weight w^A_k (applied across all levels)\n",
    "            w_A_k = self.atmos_var_weights.get(k, 1.0)  # default to 1.0 if not found\n",
    "\n",
    "            # Get number of atmospheric levels (C) for this variable\n",
    "            # Assuming pred_batch.atmos_vars[k] has shape (batch, time, C, H, W)\n",
    "            _, _, C, _, _ = pred_batch.atmos_vars[k].shape\n",
    "\n",
    "            # L1Loss with reduction='sum' sums up all absolute differences for the variable across all levels\n",
    "            # Then we normalize by C*H*W\n",
    "            var_mae = self.l1loss(pred_batch.atmos_vars[k], target_batch.atmos_vars[k]) / (C * H * W)\n",
    "            atmospheric_loss_sum += w_A_k * var_mae\n",
    "\n",
    "        # Combine weighted surface and atmospheric losses\n",
    "        combined_loss_inside_brackets = self.alpha * surface_loss_sum + self.beta * atmospheric_loss_sum\n",
    "\n",
    "        # Apply outer scaling factor\n",
    "        final_loss = (self.gamma / (VS + VA)) * combined_loss_inside_brackets\n",
    "\n",
    "        return final_loss\n",
    "\n",
    "\n",
    "class AuroraDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # xr Datasets\n",
    "        surface_ds: xr.Dataset,\n",
    "        atmospheric_ds: xr.Dataset,\n",
    "        static_ds: xr.Dataset,\n",
    "        # variables\n",
    "        surface_variables: list[str]=AURORA_VARIABLE_NAMES[\"surface\"],\n",
    "        atmospheric_variables: list[str]=AURORA_VARIABLE_NAMES[\"atmospheric\"],\n",
    "        static_variables: list[str]=AURORA_VARIABLE_NAMES[\"static\"],\n",
    "        # temporal parameters\n",
    "        base_frequency: Union[str, pd.Timedelta]=\"6h\",\n",
    "        input_temporal_length: Union[str, int, pd.Timedelta]=\"12h\",\n",
    "        output_temporal_length: Union[str, int, pd.Timedelta]=\"6h\",\n",
    "        inter_sample_gap: Union[str, int, pd.Timedelta]=\"6h\",\n",
    "        forecast_horizon: Union[str, int, pd.Timedelta]=\"6w\",\n",
    "        init_frequency: Union[str, int, pd.Timedelta]=\"1d\",\n",
    "        init_gap: Union[str, int, pd.Timedelta]=\"0h\",\n",
    "        downsampling_rate: int=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialise the AuroraDataset.\n",
    "\n",
    "        Args:\n",
    "            surface_ds: xr.Dataset\n",
    "                The surface dataset.\n",
    "            atmospheric_ds: xr.Dataset\n",
    "                The atmospheric dataset.\n",
    "            static_ds: xr.Dataset\n",
    "                The static dataset.\n",
    "            surface_variables: list[str]\n",
    "                The surface variables to include. Defaults to Aurora's surface variables.\n",
    "            atmospheric_variables: list[str]\n",
    "                The atmospheric variables to include. Defaults to Aurora's atmospheric variables.\n",
    "            static_variables: list[str]\n",
    "                The static variables to include. Defaults to Aurora's static variables.\n",
    "            base_frequency: Union[str, pd.Timedelta]\n",
    "                The base frequency of the data. Defaults to 6 hours.\n",
    "            input_temporal_length: Union[str, int, pd.Timedelta]\n",
    "                The length of the input time series. If `int`, considered to be a multiple of `base_frequency`. Defaults to 12 hours.\n",
    "            output_temporal_length: Union[str, int, pd.Timedelta]\n",
    "                The length of the output time series. If `int`, considered to be a multiple of `base_frequency`. Defaults to 6 hours.\n",
    "            inter_sample_gap: Union[str, int, pd.Timedelta]\n",
    "                The gap between consecutive samples. If `int`, considered to be a multiple of `base_frequency`. Defaults to 6 hours.\n",
    "            forecast_horizon: Union[str, int, pd.Timedelta]\n",
    "                The forecast horizon. If `int`, considered to be a multiple of `base_frequency`. Defaults to 6 weeks.\n",
    "            init_frequency: Union[str, int, pd.Timedelta]\n",
    "                The frequency of the initialisation times. If `int`, considered to be a multiple of `base_frequency`. Defaults to 1 day.\n",
    "            init_gap: Union[str, int, pd.Timedelta]\n",
    "                The gap between the initialisation time and the start of the input time series. If `int`, considered to be a multiple of \n",
    "                `base_frequency`. Defaults to 0 hours.\n",
    "            downsampling_rate: int\n",
    "                The downsampling rate of the dataset. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # get sorted datasets\n",
    "        # Aurora requires latitudes to be in descending order\n",
    "        # and longitudes to be in ascending order\n",
    "        self.surface_ds = surface_ds.sortby(\"latitude\", ascending=False).sortby(\"longitude\", ascending=True)\n",
    "        self.atmospheric_ds = atmospheric_ds.sortby(\"latitude\", ascending=False).sortby(\"longitude\", ascending=True)\n",
    "        self.static_ds = static_ds.sortby(\"latitude\", ascending=False).sortby(\"longitude\", ascending=True)\n",
    "\n",
    "        self.atmospheric_variables = atmospheric_variables\n",
    "        self.surface_variables = surface_variables\n",
    "        self.static_variables = static_variables\n",
    "\n",
    "        self.base_frequency = pd.Timedelta(base_frequency) if isinstance(base_frequency, str) else base_frequency\n",
    "        self.input_temporal_length = convert_to_steps(input_temporal_length, self.base_frequency, check_valid=True)\n",
    "        self.output_temporal_length = convert_to_steps(output_temporal_length, self.base_frequency, check_valid=True)\n",
    "        self.inter_sample_gap = convert_to_steps(inter_sample_gap, self.base_frequency, check_valid=True)\n",
    "        self.forecast_horizon = convert_to_steps(forecast_horizon, self.base_frequency, check_valid=True)\n",
    "        self.init_frequency = convert_to_steps(init_frequency, self.base_frequency, check_valid=True)\n",
    "        self.init_gap = convert_to_steps(init_gap, self.base_frequency, check_valid=True)\n",
    "\n",
    "        self.downsampling_rate = downsampling_rate\n",
    "        self._spatial_shape = None\n",
    "\n",
    "        # extract timestamps\n",
    "        assert (surface_ds.time == atmospheric_ds.time).all(), f\"got different timestamps for surface and atmospheric data.\"\n",
    "        self.timestamps = self.surface_ds.time.values.astype(\"datetime64[s]\")\n",
    "        \n",
    "        # add init gap\n",
    "        if self.init_gap < 0:\n",
    "            # reverse the init gap index based on the length of self.timestamps\n",
    "            self.init_gap = len(self.timestamps) + self.init_gap\n",
    "        self.timestamps = self.timestamps[self.init_gap:]        \n",
    "\n",
    "        # compute feasible length of the time series\n",
    "        discarded_timesteps = self.forecast_horizon + self.output_temporal_length + self.input_temporal_length + self.inter_sample_gap\n",
    "        self.num_samples = len(self.timestamps) - discarded_timesteps\n",
    "        assert self.timestamps[self.num_samples-1] + discarded_timesteps * self.base_frequency == self.timestamps[-1]\n",
    "\n",
    "        # add init frequency\n",
    "        self.num_samples = self.num_samples // self.init_frequency\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_cloud_storage(\n",
    "        cls,\n",
    "        gcs_url: str,\n",
    "        start_year: str|int=None,\n",
    "        end_year: str|int=None,\n",
    "        static_url: str=STATIC_VARS_HF_URL,\n",
    "        atmospheric_variables: list[str]=AURORA_VARIABLE_NAMES[\"atmospheric\"],\n",
    "        surface_variables: list[str]=AURORA_VARIABLE_NAMES[\"surface\"],\n",
    "        pressure_levels: list[int]=AURORA_PRESSURE_LEVELS,\n",
    "        variable_names_map: dict[str, str]=None,\n",
    "        **kwargs\n",
    "    ) -> \"AuroraDataset\":\n",
    "        \"\"\"\n",
    "        No need to download anything locally!\n",
    "\n",
    "        Args:\n",
    "            gcs_url: str\n",
    "                The url to the zarr store in the cloud storage.\n",
    "            static_url: str\n",
    "                The url to the static dataset in the cloud storage.\n",
    "            atmospheric_variables: list[str]\n",
    "                The atmospheric variables to include. Defaults to Aurora's atmospheric variables.\n",
    "            surface_variables: list[str]\n",
    "                The surface variables to include. Defaults to Aurora's surface variables.\n",
    "            pressure_levels: list[int]\n",
    "                The pressure levels to include. Defaults to Aurora's pressure levels.\n",
    "            variable_names_map: dict[str, str]\n",
    "                A dictionary mapping the variable names to the desired names.\n",
    "            **kwargs:\n",
    "                Additional arguments to pass to the AuroraDataset constructor.\n",
    "        Returns:\n",
    "            AuroraDataset\n",
    "        \"\"\"\n",
    "        static_ds = load_static_ds_local(\"static_data/static.nc\")\n",
    "\n",
    "        # load surface and atmospheric ds\n",
    "        surface_ds, atmospheric_ds = load_gcs_datasets(\n",
    "            gcs_url=gcs_url,\n",
    "            start_year=start_year,\n",
    "            end_year=end_year,\n",
    "            atmospheric_variables=atmospheric_variables,\n",
    "            surface_variables=surface_variables,\n",
    "            pressure_levels=pressure_levels,\n",
    "            variable_names_map=variable_names_map\n",
    "        )\n",
    "        return cls(\n",
    "            surface_ds=surface_ds,\n",
    "            atmospheric_ds=atmospheric_ds,\n",
    "            static_ds=static_ds,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def spatial_shape(self) -> tuple[int]:\n",
    "        \"\"\"\n",
    "        return (nlat, nlon)\n",
    "        \"\"\"\n",
    "        # return if already computed\n",
    "        if self._spatial_shape is not None:\n",
    "            return self._spatial_shape\n",
    "        # compute\n",
    "        batch = self[0][0]\n",
    "        if self.downsampling_rate is not None and self.downsampling_rate > 1:\n",
    "            with torch.no_grad():\n",
    "                batch = downsample_batch(batch, self.downsampling_rate)\n",
    "\n",
    "        _,_,H,W = batch.surf_vars[next(iter(batch.surf_vars.keys()))].shape\n",
    "        self._spatial_shape = (H, W)\n",
    "        return self._spatial_shape\n",
    "\n",
    "    def get_matching_timestamp(self, timestamp: np.datetime64|datetime) -> np.datetime64:\n",
    "        \"\"\"\n",
    "        Find a create an OUTPUT batch from the requested timestamp. \n",
    "        \"\"\"\n",
    "        if self.output_temporal_length > 1:\n",
    "            raise NotImplementedError(\"get_matching_timestamp is only implemented for output_temporal_length=1.\")\n",
    "\n",
    "        if isinstance(timestamp, datetime):\n",
    "            timestamp = np.datetime64(timestamp)\n",
    "\n",
    "        # get output slice\n",
    "        output_slice = [\n",
    "            timestamp + np.timedelta64(pd.Timedelta(self.base_frequency).value * n, \"s\")\n",
    "            for n in range(self.output_temporal_length)\n",
    "        ]\n",
    "\n",
    "        # return the batch\n",
    "        return xr_to_batch(\n",
    "            self.surface_ds.sel(time=output_slice).compute(),\n",
    "            self.atmospheric_ds.sel(time=output_slice).compute(),\n",
    "            self.static_ds.compute(),\n",
    "            surface_variables=self.surface_variables,\n",
    "            static_variables=self.static_variables,\n",
    "            atmospheric_variables=self.atmospheric_variables\n",
    "        )\n",
    "\n",
    "    def get_matching_output(self, batch: Batch) -> Batch:\n",
    "        \"\"\"\n",
    "        Creates a Batch from the requested timestamp. The timestamp is the first \n",
    "        timestamp of the output time series of length `output_temporal_length`.\n",
    "        \"\"\"\n",
    "        T = batch.atmos_vars[next(iter(batch.atmos_vars.keys()))].shape[1]\n",
    "        if T != 1:\n",
    "            raise NotImplementedError(\"get_matching_output is only implemented for T=1.\")\n",
    "\n",
    "        # get the times\n",
    "        times = list(batch.metadata.time)\n",
    "        output_batch = []\n",
    "\n",
    "        # loop over requested times\n",
    "        for time in times:\n",
    "            new = self.get_matching_timestamp(time)\n",
    "            output_batch.append(new)\n",
    "\n",
    "        # collate\n",
    "        output_batch = batch_collate_fn(output_batch)\n",
    "\n",
    "        # apply downsampling\n",
    "        if self.downsampling_rate is not None and self.downsampling_rate > 1:\n",
    "            with torch.no_grad():\n",
    "                output_batch = downsample_batch(output_batch, self.downsampling_rate)\n",
    "\n",
    "        return output_batch\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Batch:\n",
    "        if idx < 0:\n",
    "            idx = self.num_samples + idx\n",
    "\n",
    "        # add init frequency\n",
    "        idx = idx * self.init_frequency\n",
    "\n",
    "        # get input and output indexes\n",
    "        input_slice = slice(idx, idx+self.input_temporal_length)\n",
    "        output_slice = slice(idx+self.input_temporal_length+self.inter_sample_gap-1,\n",
    "                             idx+self.input_temporal_length+self.inter_sample_gap-1+self.output_temporal_length)\n",
    "\n",
    "        # get timestamps\n",
    "        input_timestamps = self.timestamps[input_slice]\n",
    "        output_timestamps = self.timestamps[output_slice]\n",
    "\n",
    "        # make a few checks\n",
    "        assert len(input_timestamps) == self.input_temporal_length\n",
    "        assert len(output_timestamps) == self.output_temporal_length\n",
    "        assert input_timestamps[-1] + self.inter_sample_gap * pd.Timedelta(self.base_frequency) == output_timestamps[0]\n",
    "\n",
    "        input_batch = xr_to_batch(\n",
    "            self.surface_ds.sel(time=input_timestamps).compute(),\n",
    "            self.atmospheric_ds.sel(time=input_timestamps).compute(),\n",
    "            self.static_ds.compute(),\n",
    "            surface_variables=self.surface_variables,\n",
    "            static_variables=self.static_variables,\n",
    "            atmospheric_variables=self.atmospheric_variables\n",
    "        )\n",
    "\n",
    "        output_batch = xr_to_batch(\n",
    "            self.surface_ds.sel(time=output_timestamps).compute(),\n",
    "            self.atmospheric_ds.sel(time=output_timestamps).compute(),\n",
    "            self.static_ds.compute(),\n",
    "            surface_variables=self.surface_variables,\n",
    "            static_variables=self.static_variables,\n",
    "            atmospheric_variables=self.atmospheric_variables\n",
    "        )\n",
    "\n",
    "        if self.downsampling_rate is not None and self.downsampling_rate > 1:\n",
    "            with torch.no_grad():\n",
    "                input_batch = downsample_batch(input_batch, self.downsampling_rate)\n",
    "                output_batch = downsample_batch(output_batch, self.downsampling_rate)\n",
    "\n",
    "        return input_batch, output_batch\n",
    "\n",
    "\n",
    "# Functions\n",
    "def normalise_surf_var(\n",
    "    x: torch.Tensor,\n",
    "    name: str,\n",
    "    stats: dict[str, tuple[float, float]],\n",
    "    unnormalise: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Normalise a surface-level variable.\"\"\"\n",
    "    location, scale = stats[name]\n",
    "    if unnormalise:\n",
    "        return x * scale + location\n",
    "    else:\n",
    "        return (x - location) / scale\n",
    "\n",
    "\n",
    "def normalise_atmos_var(\n",
    "    x: torch.Tensor,\n",
    "    name: str,\n",
    "    atmos_levels: tuple[int | float, ...],\n",
    "    stats: dict[str, tuple[float, float]],\n",
    "    unnormalise: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Normalise an atmospheric variable.\"\"\"\n",
    "    level_locations: list[int | float] = []\n",
    "    level_scales: list[int | float] = []\n",
    "    for level in atmos_levels:\n",
    "        name_level = f\"{name}_{level}\"\n",
    "        level_locations.append(stats[name_level][0])\n",
    "        level_scales.append(stats[name_level][1])\n",
    "    location = torch.tensor(level_locations, dtype=x.dtype, device=x.device)\n",
    "    scale = torch.tensor(level_scales, dtype=x.dtype, device=x.device)\n",
    "\n",
    "    if unnormalise:\n",
    "        return x * scale[..., None, None] + location[..., None, None]\n",
    "    else:\n",
    "        return (x - location[..., None, None]) / scale[..., None, None]\n",
    "\n",
    "\n",
    "unnormalise_surf_var = partial(normalise_surf_var, unnormalise=True)\n",
    "unnormalise_atmos_var = partial(normalise_atmos_var, unnormalise=True)\n",
    "\n",
    "\n",
    "def fillna_spatial_mean_conv(tensor: torch.Tensor, kernel_size: int = 3, iterations: int = 5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Approximate filling of NaNs via convolutional local mean.\n",
    "    \n",
    "    Supports:\n",
    "      - 4D (B, T, H, W)\n",
    "      - 5D (B, T, L, H, W)\n",
    "\n",
    "    Args:\n",
    "        tensor: Tensor with NaNs (on GPU or CPU).\n",
    "        kernel_size: Spatial window size (must be odd).\n",
    "        iterations: Number of fill iterations.\n",
    "\n",
    "    Returns:\n",
    "        Tensor with NaNs replaced by local mean.\n",
    "    \"\"\"\n",
    "    assert kernel_size % 2 == 1, \"kernel_size must be odd\"\n",
    "    \n",
    "    if not torch.isnan(tensor).any():\n",
    "        return tensor  # No NaNs — early exit\n",
    "\n",
    "    device = tensor.device\n",
    "    filled = tensor.clone()\n",
    "    nan_mask = torch.isnan(filled)\n",
    "    filled[nan_mask] = 0  # Replace NaNs with 0s for now\n",
    "\n",
    "    # Set up convolution kernel\n",
    "    kernel_dim = 2 if tensor.ndim == 4 else 3\n",
    "    spatial_dims = (-2, -1) if kernel_dim == 2 else (-3, -2, -1)\n",
    "    padding = kernel_size // 2\n",
    "\n",
    "    # Define kernel\n",
    "    kernel_shape = [1] * (tensor.ndim - kernel_dim) + [1] * kernel_dim\n",
    "    kernel = torch.ones(kernel_shape[:-kernel_dim] + [kernel_size] * kernel_dim, device=device)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        valid_mask = ~torch.isnan(tensor)\n",
    "        weights = valid_mask.float()\n",
    "        values = filled * weights\n",
    "\n",
    "        if kernel_dim == 2:\n",
    "            smoothed_vals = F.conv2d(values.view(-1, 1, *values.shape[-2:]), kernel, padding=padding, groups=1)\n",
    "            smoothed_weights = F.conv2d(weights.view(-1, 1, *weights.shape[-2:]), kernel, padding=padding, groups=1)\n",
    "        else:  # 3D\n",
    "            smoothed_vals = F.conv3d(values.view(-1, 1, *values.shape[-3:]), kernel, padding=padding, groups=1)\n",
    "            smoothed_weights = F.conv3d(weights.view(-1, 1, *weights.shape[-3:]), kernel, padding=padding, groups=1)\n",
    "\n",
    "        smoothed_mean = smoothed_vals / (smoothed_weights + 1e-6)\n",
    "        smoothed_mean = smoothed_mean.view_as(filled)\n",
    "\n",
    "        # Update only NaN locations\n",
    "        filled[nan_mask] = smoothed_mean[nan_mask]\n",
    "\n",
    "    return filled\n",
    "\n",
    "\n",
    "def convert_to_steps(temporal_length: Union[str, int, pd.Timedelta], \n",
    "                      base_frequency: Union[str, pd.Timedelta],\n",
    "                      check_valid: bool=True) -> int:\n",
    "    # convert to timedelta\n",
    "    if isinstance(temporal_length, str):\n",
    "        temporal_length = pd.Timedelta(temporal_length)\n",
    "    elif isinstance(temporal_length, int):\n",
    "        return temporal_length\n",
    "    # check if it is a multiple of the base frequency\n",
    "    if check_valid:\n",
    "        assert temporal_length % pd.Timedelta(base_frequency) == pd.Timedelta(0), \\\n",
    "            \"temporal_length must be a multiple of base_frequency\"\n",
    "    return int(temporal_length / base_frequency)\n",
    "\n",
    "\n",
    "def downsample_batch(batch, factor, pooling=F.avg_pool2d):\n",
    "    return batch.downsample(factor)\n",
    "\n",
    "\n",
    "def batch_collate_fn(batches: list[Batch]|None) -> Batch:\n",
    "    \"\"\"\n",
    "    Custom collate function to combine multiple Batch objects. Collating custom\n",
    "    objects such as Batch in PyTorch requires a custom function. For efficient\n",
    "    parallel processing on the GPU, individual samples are combined into a single\n",
    "    larger 'batch'. In the context of Aurora, one 'sample' is one Aurora Batch\n",
    "    instance: all the atmospheric and surface variables for a single point in\n",
    "    time at a specific location.\n",
    "    \"\"\"\n",
    "    # Check whether the input is of type Batch\n",
    "    _batches = batches.copy()\n",
    "    for i, batch in enumerate(_batches):\n",
    "        if batch is None: batches.pop(i)\n",
    "        elif not isinstance(batch, Batch):\n",
    "            raise ValueError(f\"Expected a list of Aurora batches or NoneType, got {type(batch)}\")\n",
    "    batches = _batches\n",
    "\n",
    "    if len(batches) == 0:\n",
    "        return # nothing to batch return None\n",
    "    elif len(batches) == 1:\n",
    "        return batches[0] # nothing to batch return the single batch\n",
    "\n",
    "    # Prediction batches have a single time sample apparently\n",
    "    times = []\n",
    "    for batch in batches:\n",
    "        time = batch.metadata.time\n",
    "        if isinstance(time, (tuple, list)):\n",
    "            times.extend(list(time))\n",
    "        else:\n",
    "            times.append(time)\n",
    "    times = tuple(times)\n",
    "\n",
    "    # batch the data\n",
    "    return Batch(\n",
    "        surf_vars={\n",
    "            var: torch.cat([batch.surf_vars[var] for batch in batches], dim=0)\n",
    "            for var in batches[0].surf_vars\n",
    "        },\n",
    "        atmos_vars={\n",
    "            var: torch.cat([batch.atmos_vars[var] for batch in batches], dim=0)\n",
    "            for var in batches[0].atmos_vars\n",
    "        },\n",
    "        static_vars={\n",
    "            var: batches[0].static_vars[var]\n",
    "            for var in batches[0].static_vars\n",
    "        },\n",
    "        metadata=Metadata(\n",
    "            lat=batches[0].metadata.lat,\n",
    "            lon=batches[0].metadata.lon,\n",
    "            atmos_levels=batches[0].metadata.atmos_levels,\n",
    "            rollout_step=batches[0].metadata.rollout_step,\n",
    "            time=times,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def batch_pairs_collate_fn(batch_pairs: list[tuple[Batch, Batch]]) -> tuple[Batch, Batch]:\n",
    "    \"\"\"\n",
    "    Collate function for batch input/output pairs. This function is a wrapper designed to \n",
    "    handle the common scenario in supervised learning where the DataLoader yields pairs \n",
    "    of (input, target).\n",
    "\n",
    "    Input: It expects a list where each element is a tuple containing an input Batch and a \n",
    "    target Batch. \n",
    "    E.g., [(input_batch_1, target_batch_1), (input_batch_2, target_batch_2), ...] could be \n",
    "    such a list.\n",
    "    \"\"\"\n",
    "    input_batch = batch_collate_fn([batch_pair[0] for batch_pair in batch_pairs])\n",
    "    output_batch = batch_collate_fn([batch_pair[1] for batch_pair in batch_pairs])\n",
    "\n",
    "    return input_batch, output_batch\n",
    "\n",
    "\n",
    "def prepare_lons_lats(lons: np.ndarray, lats: np.ndarray) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Aurora requires decreasing latitudes and increasing longitudes\n",
    "    \"\"\"\n",
    "    if lats[0] < lats[1]: # i.e. increasing\n",
    "        lats = torch.from_numpy(lats[::-1].copy())\n",
    "        flip_lats = True\n",
    "    else:\n",
    "        lats = torch.from_numpy(lats)\n",
    "        flip_lats = False\n",
    "\n",
    "    if lons[0] < lons[1]: # i.e. increasing\n",
    "        lons = torch.from_numpy(lons)\n",
    "        flip_lons = False\n",
    "    else:\n",
    "        lons = torch.from_numpy(lons[::-1].copy())\n",
    "        flip_lons = True\n",
    "\n",
    "    return lons, lats, flip_lats, flip_lons\n",
    "\n",
    "\n",
    "def prepare_array(x: np.ndarray, shape: tuple[int], flip_lons: bool, flip_lats: bool) -> torch.Tensor:\n",
    "    x = x.reshape(shape).copy()\n",
    "    if flip_lons: x = x[...,::-1].copy()\n",
    "    if flip_lats: x = x[...,::-1,:].copy()\n",
    "    return torch.from_numpy(x).clone()\n",
    "\n",
    "\n",
    "def rename_xr_variables(ds: xr.Dataset, variable_names_map: dict[str, str]):\n",
    "    \"\"\"\n",
    "    Directly taken from Eliot's data/utils.py.\n",
    "    \"\"\"\n",
    "    # Intersect with data_vars\n",
    "    variable_names_map = {k: v for k, v in variable_names_map.items() if k in list(ds.data_vars)+list(ds.coords)}\n",
    "\n",
    "    # Rename the variables in the dataset\n",
    "    renamed_ds = ds.rename(variable_names_map)\n",
    "\n",
    "    return renamed_ds\n",
    "\n",
    "\n",
    "def load_gcs_datasets(\n",
    "    gcs_url: str=GCS_URL,\n",
    "    num_debug_timesteps: int=0,\n",
    "    start_year: str|int=None,\n",
    "    end_year: str|int=None,\n",
    "    atmospheric_variables: list[str]=AURORA_VARIABLE_NAMES[\"atmospheric\"],\n",
    "    surface_variables: list[str]=AURORA_VARIABLE_NAMES[\"surface\"],\n",
    "    pressure_levels: list[int]=AURORA_PRESSURE_LEVELS,\n",
    "    variable_names_map: dict[str, str]=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    A modified version of the function in Eliot's data/utils.py, allowing for\n",
    "    selecting only a small number of recent time points for debugging\n",
    "    purposes.\n",
    "    \"\"\"\n",
    "    if start_year is None:\n",
    "        assert end_year is None, \"If start_year is None, end_year must be None\"\n",
    "    if end_year is None:\n",
    "        assert start_year is None, \"If end_year is None, start_year must be None\"\n",
    "\n",
    "    ds = xr.open_dataset(\n",
    "        gcs_url,\n",
    "        engine=\"zarr\",\n",
    "        chunks={},\n",
    "        storage_options={\"token\": \"anon\"},\n",
    "    )\n",
    "\n",
    "    if num_debug_timesteps:  # Debug mode: select a small number of recent time points\n",
    "        ds = ds.isel(time=slice(-num_debug_timesteps, None))\n",
    "\n",
    "        # Create a slice from the selected times\n",
    "        time_slice = slice(ds.time.values[0], ds.time.values[-1])\n",
    "    elif start_year is not None and end_year is not None:\n",
    "        start_date = f\"{start_year}-01-01\"\n",
    "        end_date = f\"{end_year}-12-31\"\n",
    "        time_slice = slice(start_date, end_date)\n",
    "    else:\n",
    "        time_slice = None\n",
    "\n",
    "    # Remove -90 from latitude (i.e. 721 -> 720)\n",
    "    # Solves ValueError: cannot reshape array of size 1036800 into shape (721,1440)\n",
    "    if 90 in ds.latitude.values and -90 in ds.latitude.values:\n",
    "        ds = ds.where(ds.latitude!=-90, drop=True)\n",
    "\n",
    "    # Get the rename map\n",
    "    if variable_names_map is None:\n",
    "        freq = pd.infer_freq(ds.time.values)\n",
    "\n",
    "        if not freq[0].isnumeric():\n",
    "            freq = \"1\" + freq\n",
    "\n",
    "        if pd.Timedelta(freq) in [pd.Timedelta(\"6h\"), pd.Timedelta(\"24h\")]:\n",
    "            variable_names_map = ERA5_HRES_T0_WB2_VARIABLE_NAMES_MAP\n",
    "        else:\n",
    "            raise ValueError(f\"No variable name map for freq {freq}\")\n",
    "\n",
    "    if len(surface_variables) > 0:\n",
    "        surface_ds = rename_xr_variables(\n",
    "            ds,\n",
    "            variable_names_map\n",
    "        )[surface_variables]\n",
    "\n",
    "        if time_slice is not None:\n",
    "            surface_ds = surface_ds.sel(time=time_slice)\n",
    "            surface_ds = surface_ds.sortby(\"latitude\", ascending=False).sortby(\"longitude\", ascending=True)\n",
    "    else:\n",
    "        surface_ds = None\n",
    "\n",
    "    if len(atmospheric_variables) > 0:\n",
    "        atmospheric_ds = rename_xr_variables(\n",
    "            ds,\n",
    "            variable_names_map\n",
    "        )[atmospheric_variables].sel(level=pressure_levels)\n",
    "\n",
    "        if time_slice is not None:\n",
    "            atmospheric_ds = atmospheric_ds.sel(time=time_slice)\n",
    "            atmospheric_ds = atmospheric_ds.sortby(\"latitude\", ascending=False).sortby(\"longitude\", ascending=True)\n",
    "    else:\n",
    "        atmospheric_ds = None\n",
    "\n",
    "    return surface_ds, atmospheric_ds\n",
    "\n",
    "\n",
    "def xr_to_batch(\n",
    "    surface_ds: xr.Dataset,\n",
    "    atmospheric_ds: xr.Dataset,\n",
    "    static_ds: xr.Dataset,\n",
    "    surface_variables: list[str]=AURORA_VARIABLE_NAMES[\"surface\"],\n",
    "    static_variables: list[str]=AURORA_VARIABLE_NAMES[\"static\"],\n",
    "    atmospheric_variables: list[str]=AURORA_VARIABLE_NAMES[\"atmospheric\"],\n",
    ") -> Batch:\n",
    "    \"\"\"\n",
    "    Create an Aurora Batch from XR Datasets.\n",
    "\n",
    "    inspired by https://microsoft.github.io/aurora/example_era5.html\n",
    "    and https://microsoft.github.io/aurora/example_hres_t0.html\n",
    "    \"\"\"\n",
    "    # Converting to `datetime64[s]` ensures that the output of `tolist()` gives\n",
    "    # `datetime.datetime`s. Note that this needs to be a tuple of length one:\n",
    "    # one value for every batch element.\n",
    "    # temporally, we want index 0 to be PREVIOUS time step and index 1 to be CURRENT time step\n",
    "    # the metadata 'time' refers to the CURRENT time step, i.e. the last index\n",
    "    if surface_ds.sizes[\"time\"] == 1:\n",
    "        _time = (surface_ds.time.values[0].astype(\"datetime64[s]\").item(), )\n",
    "    else:\n",
    "        times = list(sorted(surface_ds.time.values.astype(\"datetime64[s]\").tolist()))\n",
    "        _time = (times[-1],) # only the last\n",
    "\n",
    "    # get shapes for explicit reshaping and get lons, lats, levels\n",
    "    # the process is repeated for each dataset because the\n",
    "    # datasets can be empty\n",
    "    if static_ds is not None and len(static_ds) > 0:\n",
    "        H, W = static_ds.sizes[\"latitude\"], static_ds.sizes[\"longitude\"]\n",
    "\n",
    "        # 1. get lons, lats\n",
    "        lons = static_ds.longitude.values.copy()\n",
    "        lats = static_ds.latitude.values.copy()\n",
    "\n",
    "        # 2. prepare lons, lats\n",
    "        lons, lats, flip_lats_static, flip_lons_static = prepare_lons_lats(lons, lats)\n",
    "\n",
    "    else:\n",
    "        flip_lats_static = False\n",
    "        flip_lons_static = False\n",
    "\n",
    "    if surface_ds is not None and len(surface_ds) > 0:\n",
    "        # sort time\n",
    "        surface_ds = surface_ds.sortby(\"time\", ascending=True)\n",
    "        T, H, W = surface_ds.sizes[\"time\"], surface_ds.sizes[\"latitude\"], surface_ds.sizes[\"longitude\"]\n",
    "\n",
    "        # 1. get lons, lats\n",
    "        lons = surface_ds.longitude.values.copy()\n",
    "        lats = surface_ds.latitude.values.copy()\n",
    "\n",
    "        # 2. prepare lons, lats\n",
    "        lons, lats, flip_lats_surface, flip_lons_surface = prepare_lons_lats(lons, lats)\n",
    "    else:\n",
    "        flip_lats_surface = False\n",
    "        flip_lons_surface = False\n",
    "\n",
    "    if atmospheric_ds is not None and len(atmospheric_ds) > 0:\n",
    "        # sort time\n",
    "        atmospheric_ds = atmospheric_ds.sortby(\"time\", ascending=True)\n",
    "        C, T, H, W = (\n",
    "            atmospheric_ds.sizes[\"level\"],\n",
    "            atmospheric_ds.sizes[\"time\"],\n",
    "            atmospheric_ds.sizes[\"latitude\"],\n",
    "            atmospheric_ds.sizes[\"longitude\"],\n",
    "        )\n",
    "        # 1. get lons, lats\n",
    "        lons = atmospheric_ds.longitude.values.copy()\n",
    "        lats = atmospheric_ds.latitude.values.copy()\n",
    "\n",
    "        # 2. prepare lons, lats\n",
    "        lons, lats, flip_lats_atmospheric, flip_lons_atmospheric = prepare_lons_lats(lons, lats)\n",
    "\n",
    "        # 3. get levels\n",
    "        levels = tuple(int(level) for level in atmospheric_ds.level.values)\n",
    "    else:\n",
    "        levels = tuple()\n",
    "        flip_lats_atmospheric = False\n",
    "        flip_lons_atmospheric = False\n",
    "\n",
    "    return Batch.from_aurora_batch(\n",
    "        BaseBatch(\n",
    "            surf_vars = {\n",
    "                var: prepare_array(\n",
    "                    surface_ds[var].values,\n",
    "                    (1, T, H, W),\n",
    "                    flip_lats=flip_lats_surface,\n",
    "                    flip_lons=flip_lons_surface)\n",
    "                for var in surface_variables\n",
    "            },\n",
    "            atmos_vars = {\n",
    "                var: prepare_array(\n",
    "                    atmospheric_ds[var].values,\n",
    "                    (1, T, C, H, W),\n",
    "                    flip_lats=flip_lats_atmospheric,\n",
    "                    flip_lons=flip_lons_atmospheric)\n",
    "                for var in atmospheric_variables\n",
    "            },\n",
    "            static_vars = {\n",
    "                var: prepare_array(\n",
    "                    static_ds[var].values,\n",
    "                    (H, W),\n",
    "                    flip_lats=flip_lats_static,\n",
    "                    flip_lons=flip_lons_static)\n",
    "                for var in static_variables\n",
    "            } if static_ds is not None and len(static_ds) > 0 else {},\n",
    "            metadata=BaseMetadata(\n",
    "                lat=lats,\n",
    "                lon=lons,\n",
    "                time=_time,\n",
    "                atmos_levels=levels,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def load_static_ds_local(path: str) -> xr.Dataset:\n",
    "    ds = xr.open_dataset(path, engine=\"netcdf4\")\n",
    "    if 90 in ds.latitude.values and -90 in ds.latitude.values:\n",
    "        ds = ds.where(ds.latitude!=-90, drop=True)\n",
    "    return ds\n",
    "\n",
    "\n",
    "# Modified version of 'ERA5_HRES_T0_WB2_VARIABLE_NAMES_MAP_6HR' in Eliot's data/constants.py\n",
    "ERA5_HRES_T0_WB2_VARIABLE_NAMES_MAP = {\n",
    "    # Surface-level Variables\n",
    "    '10m_u_component_of_wind': '10u',\n",
    "    '10m_v_component_of_wind': '10v',\n",
    "    '2m_temperature': '2t',\n",
    "    'mean_sea_level_pressure': 'msl',\n",
    "\n",
    "    # Atmospheric Variables\n",
    "    'temperature': 't',\n",
    "    'u_component_of_wind': 'u',\n",
    "    'v_component_of_wind': 'v',\n",
    "    'specific_humidity': 'q',\n",
    "    'geopotential': 'z',\n",
    "}\n",
    "\n",
    "# Initialize the base (pre-trained) Aurora model\n",
    "base_model = XauroraSmall(use_lora=False, autocast=True)\n",
    "base_model.load_checkpoint(\n",
    "    \"microsoft/aurora\", \n",
    "    \"aurora-0.25-small-pretrained.ckpt\", \n",
    "    strict=False,  # to avoid error when loading state dict after disabling lora (finetuned model has lora weights)\n",
    ")\n",
    "\n",
    "names_target_modules = [\n",
    "    \"backbone.encoder_layers.0.blocks.0.attn.qkv\",\n",
    "    \"backbone.encoder_layers.0.blocks.0.attn.proj\",\n",
    "    \"backbone.encoder_layers.0.blocks.1.attn.qkv\",\n",
    "    \"backbone.encoder_layers.0.blocks.1.attn.proj\",\n",
    "    \"backbone.encoder_layers.1.blocks.0.attn.qkv\",\n",
    "    \"backbone.encoder_layers.1.blocks.0.attn.proj\",\n",
    "    \"backbone.encoder_layers.1.blocks.1.attn.qkv\",\n",
    "    \"backbone.encoder_layers.1.blocks.1.attn.proj\",\n",
    "    \"backbone.encoder_layers.1.blocks.2.attn.qkv\",\n",
    "    \"backbone.encoder_layers.1.blocks.2.attn.proj\",\n",
    "    \"backbone.encoder_layers.1.blocks.3.attn.qkv\",\n",
    "    \"backbone.encoder_layers.1.blocks.3.attn.proj\",\n",
    "    \"backbone.encoder_layers.1.blocks.4.attn.qkv\",\n",
    "    \"backbone.encoder_layers.1.blocks.4.attn.proj\",\n",
    "    \"backbone.encoder_layers.1.blocks.5.attn.qkv\",\n",
    "    \"backbone.encoder_layers.1.blocks.5.attn.proj\",\n",
    "    \"backbone.encoder_layers.2.blocks.0.attn.qkv\",\n",
    "    \"backbone.encoder_layers.2.blocks.0.attn.proj\",\n",
    "    \"backbone.encoder_layers.2.blocks.1.attn.qkv\",\n",
    "    \"backbone.encoder_layers.2.blocks.1.attn.proj\",\n",
    "    \"backbone.decoder_layers.0.blocks.0.attn.qkv\",\n",
    "    \"backbone.decoder_layers.0.blocks.0.attn.proj\",\n",
    "    \"backbone.decoder_layers.0.blocks.1.attn.qkv\",\n",
    "    \"backbone.decoder_layers.0.blocks.1.attn.proj\",\n",
    "    \"backbone.decoder_layers.1.blocks.0.attn.qkv\",\n",
    "    \"backbone.decoder_layers.1.blocks.0.attn.proj\",\n",
    "    \"backbone.decoder_layers.1.blocks.1.attn.qkv\",\n",
    "    \"backbone.decoder_layers.1.blocks.1.attn.proj\",\n",
    "    \"backbone.decoder_layers.1.blocks.2.attn.qkv\",\n",
    "    \"backbone.decoder_layers.1.blocks.2.attn.proj\",\n",
    "    \"backbone.decoder_layers.1.blocks.3.attn.qkv\",\n",
    "    \"backbone.decoder_layers.1.blocks.3.attn.proj\",\n",
    "    \"backbone.decoder_layers.1.blocks.4.attn.qkv\",\n",
    "    \"backbone.decoder_layers.1.blocks.4.attn.proj\",\n",
    "    \"backbone.decoder_layers.1.blocks.5.attn.qkv\",\n",
    "    \"backbone.decoder_layers.1.blocks.5.attn.proj\",\n",
    "    \"backbone.decoder_layers.2.blocks.0.attn.qkv\",\n",
    "    \"backbone.decoder_layers.2.blocks.0.attn.proj\",\n",
    "    \"backbone.decoder_layers.2.blocks.1.attn.qkv\",\n",
    "    \"backbone.decoder_layers.2.blocks.1.attn.proj\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "ds = xr.open_zarr(\n",
    "    \"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721.zarr/\",\n",
    "    consolidated=True,\n",
    "    storage_options={\"token\": None}\n",
    ")\n",
    "\n",
    "train_ds = ds.sel(time=slice(\"2016-01-01\", \"2020-12-31\"))\n",
    "val_ds = ds.sel(time=slice(\"2021-01-01\", \"2022-01-01\"))\n",
    "test_ds = ds.sel(time=slice(\"2022-01-01\", \"2023-01-01\"))\n",
    "\n",
    "\n",
    "dataset = AuroraDataset.from_cloud_storage(\n",
    "    gcs_url=\"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721.zarr/\",  \n",
    "    start_year=2016,\n",
    "    end_year=2023\n",
    ")\n",
    "\n",
    "# Extract datasets for regional slicing\n",
    "surface_ds = dataset.surface_ds\n",
    "atmospheric_ds = dataset.atmospheric_ds\n",
    "static_ds = dataset.static_ds\n",
    "#For fine tuning do fro 2016-2020 including 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing climatology for region: western_med...\n",
      "  → Variables: ['10u', '10v', '2t', 'msl', 't', 'u', 'v', 'q', 'z']\n",
      "  → Time range: 2021-01-01T00:00:00.000000000 to 2022-01-01T18:00:00.000000000\n",
      "  → Grid shape: (56, 48)\n",
      "  → Climatology computed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "REGIONS = {\n",
    "    \"western_med\": {\n",
    "        \"lat\": slice(45.75, 32.0),\n",
    "        \"lon\": slice(348.0, 376.0)  # -12 to 16 in 0–360 format\n",
    "    },\n",
    "    \"eastern_med\": {\n",
    "        \"lat\": slice(43.75, 30.0),\n",
    "        \"lon\": slice(16.0, 43.75)   # unchanged, direct match to east_coords\n",
    "    }\n",
    "}\n",
    "\n",
    "def compute_climatology(ds: xr.Dataset, region: str, variable_names: list[str]) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Computes daily climatology (by day-of-year) for given variables in a region.\n",
    "    Ensures consistent shape and coordinates for safe downstream evaluation.\n",
    "    \"\"\"\n",
    "    print(f\"Computing climatology for region: {region}...\")\n",
    "    # Slice region first\n",
    "    ds_region = extract_region(ds, region)\n",
    "\n",
    "    # Get day of year\n",
    "    doy = ds_region['time'].dt.dayofyear\n",
    "    print(f\"  → Variables: {variable_names}\")\n",
    "    print(f\"  → Time range: {str(ds_region.time.values[0])} to {str(ds_region.time.values[-1])}\")\n",
    "    print(f\"  → Grid shape: {ds_region[variable_names[0]].isel(time=0).shape}\")\n",
    "\n",
    "    # Compute mean for each day-of-year\n",
    "    clim = ds_region[variable_names].groupby(doy).mean('time', keep_attrs=True)\n",
    "\n",
    "    # Preserve original coordinates to avoid extract_region later\n",
    "    clim = clim.assign_coords(latitude=ds_region.latitude, longitude=ds_region.longitude)\n",
    "    print(\"  → Climatology computed.\\n\")\n",
    "    return clim\n",
    "\n",
    "\n",
    "def extract_region(ds: xr.Dataset, region_name: str) -> xr.Dataset:\n",
    "    region = REGIONS[region_name]\n",
    "    lat_slice = region[\"lat\"]\n",
    "    lon_slice = region[\"lon\"]\n",
    "\n",
    "    if isinstance(lon_slice, list):  # wraparound\n",
    "        return xr.concat(\n",
    "            [ds.sel(latitude=lat_slice, longitude=ls) for ls in lon_slice],\n",
    "            dim=\"longitude\"\n",
    "        )\n",
    "    else:\n",
    "        return ds.sel(latitude=lat_slice, longitude=lon_slice)\n",
    "        \n",
    "all_surface_vars = list(surface_ds.data_vars)\n",
    "all_atmos_vars = list(atmospheric_ds.data_vars)\n",
    "\n",
    "# Merge surface and atmospheric validation data\n",
    "val_surf = surface_ds.sel(time=slice(\"2021-01-01\", \"2022-01-01\"))\n",
    "val_atmos = atmospheric_ds.sel(time=slice(\"2021-01-01\", \"2022-01-01\"))\n",
    "val_combined = xr.merge([val_surf, val_atmos])\n",
    "\n",
    "# Compute climatology directly for region\n",
    "climatology_ds = compute_climatology(val_combined, region, all_surface_vars + all_atmos_vars)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# ------------------ UTILITY FUNCTIONS ------------------ #\n",
    "\n",
    "def move_batch_to_cpu(batch):\n",
    "    return Batch(\n",
    "        surf_vars={k: v.cpu() for k, v in batch.surf_vars.items()},\n",
    "        atmos_vars={k: v.cpu() for k, v in batch.atmos_vars.items()},\n",
    "        static_vars={k: v.cpu() for k, v in batch.static_vars.items()},\n",
    "        metadata=batch.metadata\n",
    "    )\n",
    "\n",
    "def roll_forward(input_batch: Batch, prediction: Batch) -> Batch:\n",
    "    def roll_tensor(tensor_old, tensor_new):\n",
    "        return torch.cat([tensor_old[:, 1:], tensor_new], dim=1)\n",
    "\n",
    "    new_surf = {\n",
    "        var: roll_tensor(input_batch.surf_vars[var], prediction.surf_vars[var][:, -1:])\n",
    "        for var in input_batch.surf_vars\n",
    "    }\n",
    "\n",
    "    new_atmos = {\n",
    "        var: roll_tensor(input_batch.atmos_vars[var], prediction.atmos_vars[var][:, -1:])\n",
    "        for var in input_batch.atmos_vars\n",
    "    }\n",
    "\n",
    "    new_metadata = Metadata(\n",
    "        lat=input_batch.metadata.lat,\n",
    "        lon=input_batch.metadata.lon,\n",
    "        time=(prediction.metadata.time[-1],),\n",
    "        atmos_levels=input_batch.metadata.atmos_levels,\n",
    "    )\n",
    "\n",
    "    return Batch(\n",
    "        surf_vars=new_surf,\n",
    "        atmos_vars=new_atmos,\n",
    "        static_vars=input_batch.static_vars,\n",
    "        metadata=new_metadata\n",
    "    )\n",
    "\n",
    "def get_start_batch(surface_ds, atmospheric_ds, static_ds, region: str, time: datetime) -> Batch:\n",
    "    time_window = [time - timedelta(hours=6), time]\n",
    "\n",
    "    surf_slice = surface_ds.sel(time=time_window, method=\"nearest\")\n",
    "    atmos_slice = atmospheric_ds.sel(time=time_window, method=\"nearest\")\n",
    "\n",
    "    surf_crop = extract_region(surf_slice, region)\n",
    "    atmos_crop = extract_region(atmos_slice, region)\n",
    "    static_crop = extract_region(static_ds, region)\n",
    "\n",
    "    return xr_to_batch(surf_crop, atmos_crop, static_crop)\n",
    "\n",
    "def get_ground_truth_batches(start_time, region, dataset_loader, steps=56):\n",
    "    targets = []\n",
    "    for i in range(steps):\n",
    "        t = start_time + timedelta(hours=6 * i)\n",
    "        surface_ds_t, atmospheric_ds_t, static_ds_t = dataset_loader(t)\n",
    "\n",
    "        surface_region = extract_region(surface_ds_t, region)\n",
    "        atmos_region = extract_region(atmospheric_ds_t, region)\n",
    "        static_region = extract_region(static_ds_t, region)\n",
    "\n",
    "        batch = xr_to_batch(surface_region, atmos_region, static_region)\n",
    "        targets.append(batch)\n",
    "    return targets\n",
    "\n",
    "\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "# ------------------ DATASET LOADER FUNCTION ------------------ #\n",
    "def dataset_loader(t: datetime):\n",
    "    time_window = [t - timedelta(hours=6), t]\n",
    "    surface_ds_t = surface_ds.sel(time=time_window, method=\"nearest\", tolerance=np.timedelta64(1, \"h\"))\n",
    "    atmospheric_ds_t = atmospheric_ds.sel(time=time_window, method=\"nearest\", tolerance=np.timedelta64(1, \"h\"))\n",
    "    return surface_ds_t, atmospheric_ds_t, static_ds\n",
    "\n",
    "\n",
    "# ------------------ EVALUATION FUNCTION ------------------ #\n",
    "@torch.no_grad()\n",
    "def evaluate_14_day_rollout(model, start_batch, get_ground_truth_batches_fn, climatology_ds, patch_size=16, tolerance=2.0):\n",
    "    model.eval()\n",
    "    step = timedelta(hours=6)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    predictions = []\n",
    "    current_input = deepcopy(start_batch).to(device)\n",
    "\n",
    "    for _ in range(56):\n",
    "        pred = model(current_input, step)\n",
    "        pred = pred.crop(patch_size)\n",
    "        predictions.append(move_batch_to_cpu(pred))\n",
    "        current_input = roll_forward(current_input, pred)\n",
    "\n",
    "    targets = get_ground_truth_batches_fn()\n",
    "\n",
    "    def init_dict(vars_): return {k: [] for k in vars_}\n",
    "    surface_vars = predictions[0].surf_vars.keys()\n",
    "    atmos_vars = predictions[0].atmos_vars.keys()\n",
    "\n",
    "    rmse_surf, rmse_atmos = init_dict(surface_vars), init_dict(atmos_vars)\n",
    "    acc_surf, acc_atmos = init_dict(surface_vars), init_dict(atmos_vars)\n",
    "    acc_rate_surf, acc_rate_atmos = init_dict(surface_vars), init_dict(atmos_vars)\n",
    "\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        doy = target.metadata.time[-1].timetuple().tm_yday\n",
    "\n",
    "        for k in surface_vars:\n",
    "            y_pred = pred.surf_vars[k].numpy().ravel()\n",
    "            y_true = target.surf_vars[k][:, -1].numpy().ravel()\n",
    "            rmse_surf[k].append(root_mean_squared_error(y_true, y_pred))\n",
    "\n",
    "            climatology = climatology_ds[k].sel(dayofyear=doy).values.ravel()\n",
    "\n",
    "            anomaly_pred = y_pred - climatology\n",
    "            anomaly_true = y_true - climatology\n",
    "            acc_val = np.corrcoef(anomaly_pred, anomaly_true)[0, 1] if anomaly_true.std() > 0 else 0.0\n",
    "            acc_surf[k].append(acc_val)\n",
    "            acc_rate_surf[k].append(np.mean(np.abs(y_pred - y_true) < tolerance))\n",
    "\n",
    "        for k in atmos_vars:\n",
    "            y_pred = pred.atmos_vars[k].numpy().ravel()\n",
    "            y_true = target.atmos_vars[k][:, -1].numpy().ravel()\n",
    "            rmse_atmos[k].append(root_mean_squared_error(y_true, y_pred))\n",
    "\n",
    "            climatology = climatology_ds[k].sel(dayofyear=doy).values.ravel()\n",
    "            anomaly_pred = y_pred - climatology\n",
    "            anomaly_true = y_true - climatology\n",
    "            acc_val = np.corrcoef(anomaly_pred, anomaly_true)[0, 1] if anomaly_true.std() > 0 else 0.0\n",
    "            acc_atmos[k].append(acc_val)\n",
    "            acc_rate_atmos[k].append(np.mean(np.abs(y_pred - y_true) < tolerance))\n",
    "\n",
    "    def avg_dict(d): return {k: float(np.mean(v)) for k, v in d.items()}\n",
    "\n",
    "    return {\n",
    "        \"rmse\": {\"surf\": avg_dict(rmse_surf), \"atmos\": avg_dict(rmse_atmos)},\n",
    "        \"acc\": {\"surf\": avg_dict(acc_surf), \"atmos\": avg_dict(acc_atmos)},\n",
    "        \"accuracy\": {\"surf\": avg_dict(acc_rate_surf), \"atmos\": avg_dict(acc_rate_atmos)}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Model loaded and moved to GPU.\n",
      "Region: western_med, Start Time: 2022-01-01 00:00:00\n",
      "Generating start batch...\n",
      "Start batch ready.\n",
      "Preparing ground truth batches loader...\n",
      "Running 14-day evaluation rollout...\n",
      "Evaluation complete. Displaying results...\n",
      "\n",
      "RMSE\n",
      "  Surf\n",
      "     10u: 4.0450\n",
      "     10v: 3.5327\n",
      "      2t: 5.4448\n",
      "     msl: 752.9736\n",
      "  Atmos\n",
      "       t: 4.2945\n",
      "       u: 10.2784\n",
      "       v: 10.2324\n",
      "       q: 0.0014\n",
      "       z: 831.9116\n",
      "\n",
      "ACC\n",
      "  Surf\n",
      "     10u: 0.7647\n",
      "     10v: 0.6470\n",
      "      2t: 0.4697\n",
      "     msl: 0.5525\n",
      "  Atmos\n",
      "       t: 0.7662\n",
      "       u: 0.8139\n",
      "       v: 0.6651\n",
      "       q: 0.5514\n",
      "       z: 0.7574\n",
      "\n",
      "ACCURACY\n",
      "  Surf\n",
      "     10u: 0.4767\n",
      "     10v: 0.5015\n",
      "      2t: 0.3348\n",
      "     msl: 0.0012\n",
      "  Atmos\n",
      "       t: 0.4105\n",
      "       u: 0.2047\n",
      "       v: 0.2119\n",
      "       q: 1.0000\n",
      "       z: 0.0017\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing model...\")\n",
    "base = Xaurora(use_lora=False, autocast=True)\n",
    "base.load_checkpoint(\"microsoft/aurora\", \"aurora-0.25-pretrained.ckpt\", strict=False)\n",
    "base.eval().cuda()\n",
    "print(\"Model loaded and moved to GPU.\")\n",
    "\n",
    "region = \"western_med\"\n",
    "start_time = datetime(2022, 1, 1, 0)\n",
    "print(f\"Region: {region}, Start Time: {start_time}\")\n",
    "\n",
    "print(\"Generating start batch...\")\n",
    "start_batch = get_start_batch(surface_ds, atmospheric_ds, static_ds, region, start_time)\n",
    "print(\"Start batch ready.\")\n",
    "\n",
    "print(\"Preparing ground truth batches loader...\")\n",
    "get_gt_batches = lambda: get_ground_truth_batches(start_time, region, dataset_loader)\n",
    "\n",
    "print(\"Running 14-day evaluation rollout...\")\n",
    "results = evaluate_14_day_rollout(base, start_batch, get_gt_batches, climatology_ds=climatology_ds, patch_size=8)\n",
    "\n",
    "print(\"Evaluation complete. Displaying results...\")\n",
    "\n",
    "for metric_name in [\"rmse\", \"acc\", \"accuracy\"]:\n",
    "    print(f\"\\n{metric_name.upper()}\")\n",
    "    for category in [\"surf\", \"atmos\"]:\n",
    "        print(f\"  {category.capitalize()}\")\n",
    "        for var, value in results[metric_name][category].items():\n",
    "            print(f\"    {var:>4}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aurora-envi)",
   "language": "python",
   "name": "aurora-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
